from bs4 import BeautifulSoup
import requests, nltk
import pandas as pd

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer

stopwords_ru = stopwords.words("russian")
stopwords_ru[0:11]
stemmer = SnowballStemmer("russian")

nltk.download('punkt_tab')

base_url = 'https://sonko.rkomi.ru/projects/{}'

n = 1000

def fetch_text(url):
    response = requests.get(url, verify=False)
    response.encoding = 'utf-8'
    return response.text

data = set()
for i in range(1, n + 1):
    url = base_url.format(i)
    html = fetch_text(url)
    soup = BeautifulSoup(html, 'html.parser')
    if soup.find(lambda tag:  tag.name=="h3" and "Краткое описание проекта" in tag.text):
        para_title = soup.find(lambda tag:  tag.name=="h3" and "Краткое описание проекта" in tag.text)
        paragraph = para_title.find_next("div")
        
        paragraph = paragraph.text
        words = nltk.word_tokenize(paragraph)
        words = [word.lower() for word in words if word.isalpha()]
        words = [word for word in words if word not in stopwords.words('russian')]
        pieces = [stemmer.stem(word) for word in words]
        pieces = (' '.join(pieces))
        status = soup.find("div", class_="projects-detail__status")
        
        for piece in pieces:
            data.add((pieces, status.text))

df = pd.DataFrame(list(data), columns=['pieces', 'status'])

df.to_csv('dataset.csv', index=False)
